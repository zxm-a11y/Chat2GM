2024-03-10 17:31:32,354 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-10 17:31:32,355 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-10 17:31:39,519 - root - INFO - Rendering index page with 33 chat history records
2024-03-10 17:31:39,527 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 17:31:39] "GET / HTTP/1.1" 200 -
2024-03-10 17:31:46,317 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-10 17:31:46,319 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 17:31:46] "POST /upload_file HTTP/1.1" 200 -
2024-03-10 17:31:53,702 - root - INFO - Received message: ÎÄ¼þÖÐÓÐ¶àÉÙÊý¾Ý
2024-03-10 17:31:54,160 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-10 17:31:54,160 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-10 17:31:57,175 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 17:31:59,891 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 17:32:00,347 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 17:32:00] "POST /send_message HTTP/1.1" 200 -
2024-03-10 17:33:26,034 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-10 17:33:26,035 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-10 17:33:29,914 - root - INFO - Rendering index page with 34 chat history records
2024-03-10 17:33:29,919 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 17:33:29] "GET / HTTP/1.1" 200 -
2024-03-10 17:33:35,279 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-10 17:33:35,281 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 17:33:35] "POST /upload_file HTTP/1.1" 200 -
2024-03-10 17:33:44,676 - root - INFO - Received message: ÎÄ¼þÖÐÓÐ¶àÉÙÌõÊý¾Ý
2024-03-10 17:33:45,079 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-10 17:33:45,079 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-10 17:33:50,270 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 17:33:53,360 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 17:33:53,807 - root - INFO - Received message: There are 12 rows in the dataframe.
2024-03-10 17:33:53,818 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 17:33:53] "POST /send_message HTTP/1.1" 200 -
2024-03-10 17:36:52,417 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-10 17:36:52,418 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-10 17:36:52,818 - root - INFO - Rendering index page with 35 chat history records
2024-03-10 17:36:52,823 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 17:36:52] "GET / HTTP/1.1" 200 -
2024-03-10 17:37:25,199 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-10 17:37:25,201 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 17:37:25] "POST /upload_file HTTP/1.1" 200 -
2024-03-10 17:37:31,525 - root - INFO - Received message: Çë·ÖÎöÎÄ¼þÖÐµÄÊý¾Ý
2024-03-10 17:37:31,941 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-10 17:37:31,941 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-10 17:37:35,265 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 17:37:39,722 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 17:37:42,632 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-10 17:37:42,634 - openai._base_client - INFO - Retrying request to /chat/completions in 25.320000 seconds
2024-03-10 17:38:10,873 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 17:38:13,174 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-10 17:38:13,175 - openai._base_client - INFO - Retrying request to /chat/completions in 28.830000 seconds
2024-03-10 17:38:44,957 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 17:38:51,818 - root - INFO - Received message: The dataframe contains measurements for 116 different types of bacteria across 12 samples. The samples are evenly divided into two groups or classifications. There are no missing values in the dataframe. The data for each type of bacteria varies widely, as indicated by the large standard deviations.
2024-03-10 17:38:51,827 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 17:38:51] "POST /send_message HTTP/1.1" 200 -
2024-03-10 19:49:20,546 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-10 19:49:20,547 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-10 19:49:26,909 - root - INFO - Rendering index page with 36 chat history records
2024-03-10 19:49:26,914 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 19:49:26] "GET / HTTP/1.1" 200 -
2024-03-10 19:49:40,553 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-10 19:49:40,555 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 19:49:40] "POST /upload_file HTTP/1.1" 200 -
2024-03-10 19:49:48,236 - root - INFO - Received message: ÎÄ¼þÖÐÓÐ¶àÉÙÐÐÊý¾Ý
2024-03-10 19:49:48,640 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-10 19:49:48,640 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-10 19:49:51,912 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 19:49:52,881 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 19:49:54,669 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 19:49:55,699 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 19:49:57,620 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 19:49:58,774 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 19:50:00,476 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 19:50:01,749 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 19:50:02,928 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 19:50:03,248 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 78, in send_message
    answer = chat_with_csv_agent(message)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 95, in chat_with_csv_agent
    final_thought_process = agent._construct_scratchpad(res)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'AgentExecutor' object has no attribute '_construct_scratchpad'
2024-03-10 19:50:03,251 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 19:50:03] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-10 20:07:57,827 - root - INFO - Rendering index page with 36 chat history records
2024-03-10 20:07:57,828 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 20:07:57] "GET / HTTP/1.1" 200 -
2024-03-10 20:08:04,989 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-10 20:08:04,990 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 20:08:04] "POST /upload_file HTTP/1.1" 200 -
2024-03-10 20:08:14,290 - root - INFO - Received message: ÎÄ¼þÖÐÓÐ¶àÉÙÐÐÊý¾Ý
2024-03-10 20:08:18,476 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 20:08:19,837 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-10 20:08:20,083 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 78, in send_message
    answer = chat_with_csv_agent(message)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 95, in chat_with_csv_agent
    # final_thought_process = agent._construct_scratchpad(res)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'AgentExecutor' object has no attribute '_construct_scratchpad'
2024-03-10 20:08:20,087 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 20:08:20] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-10 20:09:10,161 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-10 20:09:10,161 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-10 20:09:10,690 - root - INFO - Rendering index page with 36 chat history records
2024-03-10 20:09:10,695 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 20:09:10] "GET / HTTP/1.1" 200 -
2024-03-10 20:09:17,774 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-10 20:09:17,775 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 20:09:17] "POST /upload_file HTTP/1.1" 200 -
2024-03-10 20:09:30,279 - root - INFO - Received message: ÎÄ¼þÖÐÓÐ¶àÉÙÐÐÊý¾Ý
2024-03-10 20:09:30,668 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-10 20:09:30,673 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-10 20:09:31,182 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 78, in send_message
    answer = chat_with_csv_agent(message)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 93, in chat_with_csv_agent
    print(agent.agent.llm_chain.prompt.template)
          ^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RunnableAgent' object has no attribute 'llm_chain'
2024-03-10 20:09:31,183 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 20:09:31] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-10 20:11:00,107 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-10 20:11:00,107 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-10 20:11:04,677 - root - INFO - Rendering index page with 36 chat history records
2024-03-10 20:11:04,682 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 20:11:04] "GET / HTTP/1.1" 200 -
2024-03-10 20:11:09,536 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-10 20:11:09,537 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 20:11:09] "POST /upload_file HTTP/1.1" 200 -
2024-03-10 20:11:18,579 - root - INFO - Received message: ÎÄ¼þÖÐÓÐ¶àÉÙÐÐÊý¾Ý
2024-03-10 20:11:19,009 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-10 20:11:19,010 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-10 20:11:19,515 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 80, in send_message
    answer = chat_with_csv_agent(message)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 95, in chat_with_csv_agent
    print(agent.agent.llm_chain.prompt.template)
          ^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RunnableAgent' object has no attribute 'llm_chain'
2024-03-10 20:11:19,524 - werkzeug - INFO - 127.0.0.1 - - [10/Mar/2024 20:11:19] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-13 20:10:53,068 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-13 20:10:53,069 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-13 20:10:55,767 - root - INFO - Rendering index page with 36 chat history records
2024-03-13 20:10:55,779 - werkzeug - INFO - 127.0.0.1 - - [13/Mar/2024 20:10:55] "GET / HTTP/1.1" 200 -
2024-03-13 20:10:55,786 - root - INFO - Rendering index page with 36 chat history records
2024-03-13 20:10:55,788 - werkzeug - INFO - 127.0.0.1 - - [13/Mar/2024 20:10:55] "GET / HTTP/1.1" 200 -
2024-03-13 20:11:01,298 - werkzeug - INFO - 127.0.0.1 - - [13/Mar/2024 20:11:01] "[33mGET /favicon.ico HTTP/1.1[0m" 404 -
2024-03-13 20:11:50,088 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-13 20:11:50,089 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-13 20:12:05,489 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-13 20:12:05,490 - werkzeug - INFO - 127.0.0.1 - - [13/Mar/2024 20:12:05] "POST /upload_file HTTP/1.1" 200 -
2024-03-13 20:12:13,867 - root - INFO - Received message: ÎÄ¼þÖÐÓÐ¶àÉÙÐÐÊý¾Ý
2024-03-13 20:12:14,544 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-13 20:12:14,544 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-13 20:12:17,440 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-13 20:12:19,141 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-13 20:12:19,488 - root - INFO - Received message: 12
2024-03-13 20:12:19,501 - werkzeug - INFO - 127.0.0.1 - - [13/Mar/2024 20:12:19] "POST /send_message HTTP/1.1" 200 -
2024-03-13 21:28:44,428 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-13 21:28:44,428 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-13 21:28:47,856 - root - INFO - Rendering index page with 37 chat history records
2024-03-13 21:28:47,861 - werkzeug - INFO - 127.0.0.1 - - [13/Mar/2024 21:28:47] "GET / HTTP/1.1" 200 -
2024-03-13 21:29:28,073 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-13 21:29:28,074 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-13 21:29:29,865 - root - INFO - Rendering index page with 37 chat history records
2024-03-13 21:29:29,870 - werkzeug - INFO - 127.0.0.1 - - [13/Mar/2024 21:29:29] "GET / HTTP/1.1" 200 -
2024-03-14 10:26:49,276 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-14 10:26:49,276 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-14 10:26:56,560 - root - INFO - Rendering index page with 37 chat history records
2024-03-14 10:26:56,568 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:26:56] "GET / HTTP/1.1" 200 -
2024-03-14 10:27:03,682 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-14 10:27:03,683 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:27:03] "POST /upload_file HTTP/1.1" 200 -
2024-03-14 10:29:16,207 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-14 10:29:16,207 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-14 10:29:20,887 - root - INFO - Rendering index page with 37 chat history records
2024-03-14 10:29:20,893 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:29:20] "GET / HTTP/1.1" 200 -
2024-03-14 10:29:25,757 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-14 10:29:25,758 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:29:25] "POST /upload_file HTTP/1.1" 200 -
2024-03-14 10:29:41,039 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-14 10:29:41,039 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-14 10:29:41,052 - root - INFO - Rendering index page with 37 chat history records
2024-03-14 10:29:41,057 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:29:41] "GET / HTTP/1.1" 200 -
2024-03-14 10:29:46,123 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-14 10:29:46,124 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:29:46] "POST /upload_file HTTP/1.1" 200 -
2024-03-14 10:30:07,401 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-14 10:30:07,402 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-14 10:30:07,804 - root - INFO - Rendering index page with 37 chat history records
2024-03-14 10:30:07,809 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:30:07] "GET / HTTP/1.1" 200 -
2024-03-14 10:30:11,774 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-14 10:30:11,775 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:30:11] "POST /upload_file HTTP/1.1" 200 -
2024-03-14 10:30:53,237 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-14 10:30:53,237 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-14 10:30:53,901 - root - INFO - Rendering index page with 37 chat history records
2024-03-14 10:30:53,906 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:30:53] "GET / HTTP/1.1" 200 -
2024-03-14 10:30:58,139 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-14 10:30:58,141 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:30:58] "POST /upload_file HTTP/1.1" 200 -
2024-03-14 10:31:26,650 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-14 10:31:26,651 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-14 10:31:28,078 - root - INFO - Rendering index page with 37 chat history records
2024-03-14 10:31:28,084 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:31:28] "GET / HTTP/1.1" 200 -
2024-03-14 10:31:32,370 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-14 10:31:32,371 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:31:32] "POST /upload_file HTTP/1.1" 200 -
2024-03-14 10:31:59,729 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-14 10:31:59,730 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-14 10:32:00,572 - root - INFO - Rendering index page with 37 chat history records
2024-03-14 10:32:00,577 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:32:00] "GET / HTTP/1.1" 200 -
2024-03-14 10:32:05,350 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-14 10:32:05,351 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:32:05] "POST /upload_file HTTP/1.1" 200 -
2024-03-14 10:35:42,866 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-14 10:35:42,867 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-14 10:35:46,834 - root - INFO - Rendering index page with 37 chat history records
2024-03-14 10:35:46,839 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:35:46] "GET / HTTP/1.1" 200 -
2024-03-14 10:35:46,849 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:35:46] "[33mGET /img/upload.png HTTP/1.1[0m" 404 -
2024-03-14 10:36:24,888 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-14 10:36:24,888 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-14 10:36:26,384 - root - INFO - Rendering index page with 37 chat history records
2024-03-14 10:36:26,389 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:36:26] "GET / HTTP/1.1" 200 -
2024-03-14 10:36:26,400 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:36:26] "[33mGET /img/upload.png HTTP/1.1[0m" 404 -
2024-03-14 10:37:47,323 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-14 10:37:47,323 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-14 10:37:50,343 - root - INFO - Rendering index page with 37 chat history records
2024-03-14 10:37:50,348 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:37:50] "GET / HTTP/1.1" 200 -
2024-03-14 10:37:50,359 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:37:50] "[33mGET /upload.png HTTP/1.1[0m" 404 -
2024-03-14 10:38:20,133 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-14 10:38:20,134 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-14 10:38:31,945 - root - INFO - Rendering index page with 37 chat history records
2024-03-14 10:38:31,951 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:38:31] "GET / HTTP/1.1" 200 -
2024-03-14 10:38:31,962 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:38:31] "[33mGET /upload.png HTTP/1.1[0m" 404 -
2024-03-14 10:40:21,096 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-14 10:40:21,096 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-14 10:40:23,939 - root - INFO - Rendering index page with 37 chat history records
2024-03-14 10:40:23,944 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:40:23] "GET / HTTP/1.1" 200 -
2024-03-14 10:41:00,009 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-14 10:41:00,009 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-14 10:41:01,792 - root - INFO - Rendering index page with 37 chat history records
2024-03-14 10:41:01,797 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:41:01] "GET / HTTP/1.1" 200 -
2024-03-14 10:41:31,728 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-14 10:41:31,728 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-14 10:41:31,924 - root - INFO - Rendering index page with 37 chat history records
2024-03-14 10:41:31,929 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:41:31] "GET / HTTP/1.1" 200 -
2024-03-14 10:41:52,408 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-14 10:41:52,409 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-14 10:41:53,357 - root - INFO - Rendering index page with 37 chat history records
2024-03-14 10:41:53,362 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:41:53] "GET / HTTP/1.1" 200 -
2024-03-14 10:43:32,243 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-14 10:43:32,243 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-14 10:43:32,357 - root - INFO - Rendering index page with 37 chat history records
2024-03-14 10:43:32,361 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:43:32] "GET / HTTP/1.1" 200 -
2024-03-14 10:43:44,233 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-14 10:43:44,234 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-14 10:43:47,836 - root - INFO - Rendering index page with 37 chat history records
2024-03-14 10:43:47,841 - werkzeug - INFO - 127.0.0.1 - - [14/Mar/2024 10:43:47] "GET / HTTP/1.1" 200 -
2024-03-15 16:03:36,430 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 16:03:36,430 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 16:03:41,389 - root - INFO - Rendering index page with 37 chat history records
2024-03-15 16:03:41,415 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:03:41] "GET / HTTP/1.1" 200 -
2024-03-15 16:03:41,426 - root - INFO - Rendering index page with 37 chat history records
2024-03-15 16:03:41,429 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:03:41] "GET / HTTP/1.1" 200 -
2024-03-15 16:03:53,441 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-15 16:03:53,443 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:03:53] "POST /upload_file HTTP/1.1" 200 -
2024-03-15 16:04:47,071 - root - INFO - Received message: Çë¶ÔÎÄ¼þÖÐµÄÊý¾Ý½øÐÐ¦Á¶àÑùÐÔ·ÖÎö£¬Êä³öShannonÖ¸Êý
2024-03-15 16:05:10,071 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-15 16:05:10,072 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-15 16:05:17,957 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:05:20,123 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:05:22,600 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:05:25,029 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:05:27,282 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:05:29,326 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:05:31,166 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:05:33,534 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:05:35,586 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:05:37,827 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:05:38,343 - root - INFO - Received message: Unable to calculate the Shannon index directly using the current method.
2024-03-15 16:05:38,352 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:05:38] "POST /send_message HTTP/1.1" 200 -
2024-03-15 16:06:49,706 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 16:06:49,707 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 16:06:56,493 - root - INFO - Rendering index page with 38 chat history records
2024-03-15 16:06:56,507 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:06:56] "GET / HTTP/1.1" 200 -
2024-03-15 16:07:03,951 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-15 16:07:03,953 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:07:03] "POST /upload_file HTTP/1.1" 200 -
2024-03-15 16:07:10,389 - root - INFO - Received message: Çë¶ÔÎÄ¼þÖÐµÄÊý¾Ý½øÐÐ¦Á¶àÑùÐÔ·ÖÎö£¬Êä³öShannonÖ¸Êý
2024-03-15 16:07:10,946 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-15 16:07:10,947 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-15 16:07:14,603 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:07:20,230 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:07:24,299 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:07:24,300 - openai._base_client - INFO - Retrying request to /chat/completions in 21.798000 seconds
2024-03-15 16:08:00,280 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:08:03,759 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:08:03,760 - openai._base_client - INFO - Retrying request to /chat/completions in 15.840000 seconds
2024-03-15 16:08:21,813 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:08:25,887 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:08:25,889 - openai._base_client - INFO - Retrying request to /chat/completions in 27.834000 seconds
2024-03-15 16:08:55,873 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:09:00,131 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:09:00,132 - openai._base_client - INFO - Retrying request to /chat/completions in 28.350000 seconds
2024-03-15 16:09:30,595 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:09:33,713 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:09:33,714 - openai._base_client - INFO - Retrying request to /chat/completions in 30.137000 seconds
2024-03-15 16:10:06,059 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:10:09,630 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:10:09,632 - openai._base_client - INFO - Retrying request to /chat/completions in 30.197000 seconds
2024-03-15 16:11:11,560 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 16:11:11,561 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 16:11:15,189 - root - INFO - Rendering index page with 38 chat history records
2024-03-15 16:11:15,199 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:11:15] "GET / HTTP/1.1" 200 -
2024-03-15 16:11:25,739 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-15 16:11:25,741 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:11:25] "POST /upload_file HTTP/1.1" 200 -
2024-03-15 16:11:33,729 - root - INFO - Received message: ÎÄ¼þÖÐÓÐ¶àÉÙÊý¾Ý
2024-03-15 16:11:34,302 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-15 16:11:34,303 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-15 16:11:40,032 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:11:42,287 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:11:42,994 - root - INFO - Received message: There are 12 rows of data in the dataframe.
2024-03-15 16:11:43,025 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:11:43] "POST /send_message HTTP/1.1" 200 -
2024-03-15 16:11:48,207 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾Ý½øÐÐ¦Á¶àÑùÐÔ·ÖÎö£¬²¢Êä³öÆäshannonÖ¸Êý
2024-03-15 16:12:01,069 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:12:01,070 - openai._base_client - INFO - Retrying request to /chat/completions in 9.108000 seconds
2024-03-15 16:12:13,515 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:12:19,147 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:12:19,148 - openai._base_client - INFO - Retrying request to /chat/completions in 25.092000 seconds
2024-03-15 16:12:57,553 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:12:59,309 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:12:59,310 - openai._base_client - INFO - Retrying request to /chat/completions in 17.652000 seconds
2024-03-15 16:13:19,257 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:13:23,159 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:13:23,160 - openai._base_client - INFO - Retrying request to /chat/completions in 28.098000 seconds
2024-03-15 16:14:06,266 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:14:11,202 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:14:11,203 - openai._base_client - INFO - Retrying request to /chat/completions in 15.732000 seconds
2024-03-15 16:14:29,099 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:14:32,998 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:14:32,999 - openai._base_client - INFO - Retrying request to /chat/completions in 30.474000 seconds
2024-03-15 16:15:16,930 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:15:20,275 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:15:20,276 - openai._base_client - INFO - Retrying request to /chat/completions in 20.016000 seconds
2024-03-15 16:15:42,631 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:15:45,697 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:15:45,698 - openai._base_client - INFO - Retrying request to /chat/completions in 26.928000 seconds
2024-03-15 16:16:14,577 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:16:17,958 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:16:17,959 - openai._base_client - INFO - Retrying request to /chat/completions in 27.498000 seconds
2024-03-15 16:16:59,229 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:17:02,440 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:17:02,442 - openai._base_client - INFO - Retrying request to /chat/completions in 21.216000 seconds
2024-03-15 16:17:25,951 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:17:28,832 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:17:28,833 - openai._base_client - INFO - Retrying request to /chat/completions in 28.452000 seconds
2024-03-15 16:17:59,535 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:18:03,420 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:18:03,421 - openai._base_client - INFO - Retrying request to /chat/completions in 33.132000 seconds
2024-03-15 16:18:38,453 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:18:41,778 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:18:41,779 - openai._base_client - INFO - Retrying request to /chat/completions in 29.340000 seconds
2024-03-15 16:19:13,078 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:19:16,645 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:19:16,646 - openai._base_client - INFO - Retrying request to /chat/completions in 29.514000 seconds
2024-03-15 16:19:26,218 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 16:19:26,218 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 16:19:29,679 - root - INFO - Rendering index page with 39 chat history records
2024-03-15 16:19:29,689 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:19:29] "GET / HTTP/1.1" 200 -
2024-03-15 16:19:35,561 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-15 16:19:35,563 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:19:35] "POST /upload_file HTTP/1.1" 200 -
2024-03-15 16:19:44,315 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾Ý½øÐÐ¦Á¶àÑùÐÔ·ÖÎö£¬²¢Êä³öÆäshannonÖ¸Êý
2024-03-15 16:19:44,830 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-15 16:19:44,831 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-15 16:19:59,048 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:20:06,534 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:20:06,535 - openai._base_client - INFO - Retrying request to /chat/completions in 6.132000 seconds
2024-03-15 16:20:15,128 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:20:19,691 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:20:19,692 - openai._base_client - INFO - Retrying request to /chat/completions in 27.234000 seconds
2024-03-15 16:20:47,820 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 16:20:47,820 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 16:20:50,515 - root - INFO - Rendering index page with 39 chat history records
2024-03-15 16:20:50,523 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:20:50] "GET / HTTP/1.1" 200 -
2024-03-15 16:20:55,576 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-15 16:20:55,578 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:20:55] "POST /upload_file HTTP/1.1" 200 -
2024-03-15 16:21:02,044 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾Ý½øÐÐ¦Á¶àÑùÐÔ·ÖÎö£¬²¢Êä³öÆäshannonÖ¸Êý
2024-03-15 16:21:02,580 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-15 16:21:02,581 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-15 16:21:05,203 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:21:10,728 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:21:10,729 - openai._base_client - INFO - Retrying request to /chat/completions in 7.938000 seconds
2024-03-15 16:21:20,282 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:21:22,927 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:21:22,928 - openai._base_client - INFO - Retrying request to /chat/completions in 24.630000 seconds
2024-03-15 16:22:00,746 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:22:04,633 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:22:04,634 - openai._base_client - INFO - Retrying request to /chat/completions in 17.328000 seconds
2024-03-15 16:22:24,923 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:22:29,155 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:22:29,156 - openai._base_client - INFO - Retrying request to /chat/completions in 28.068000 seconds
2024-03-15 16:23:01,309 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:23:04,691 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:23:04,692 - openai._base_client - INFO - Retrying request to /chat/completions in 28.392000 seconds
2024-03-15 16:23:35,410 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:23:38,904 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:23:38,905 - openai._base_client - INFO - Retrying request to /chat/completions in 30.702000 seconds
2024-03-15 16:23:52,392 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 16:23:52,392 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 16:23:56,651 - root - INFO - Rendering index page with 39 chat history records
2024-03-15 16:23:56,660 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:23:56] "GET / HTTP/1.1" 200 -
2024-03-15 16:24:07,564 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-15 16:24:07,566 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:24:07] "POST /upload_file HTTP/1.1" 200 -
2024-03-15 16:24:42,256 - root - INFO - Received message: Çë¶ÔÎÄ¼þÖÐµÄÊý¾Ý½øÐÐ¦Â·ÖÎö£¬¼ÆËãBray-Curtis¾àÀë
2024-03-15 16:24:42,802 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-15 16:24:42,803 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-15 16:24:45,685 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:24:49,576 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:25:09,705 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:25:09,706 - openai._base_client - INFO - Retrying request to /chat/completions in 7.668000 seconds
2024-03-15 16:25:19,133 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:25:20,201 - root - INFO - Received message: The Bray-Curtis distance of the data has been calculated.
2024-03-15 16:25:20,230 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:25:20] "POST /send_message HTTP/1.1" 200 -
2024-03-15 16:25:48,447 - root - INFO - Received message: Çë¶ÔÎÄ¼þÖÐµÄÊý¾Ý½øÐÐ¦Â·ÖÎö£¬¼ÆËãBray-Curtis¾àÀë£¬²¢Êä³öBray-Curtis¾àÀë
2024-03-15 16:25:50,707 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:25:56,610 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:25:56,612 - openai._base_client - INFO - Retrying request to /chat/completions in 20.958000 seconds
2024-03-15 16:26:19,673 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:26:23,578 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:26:23,580 - openai._base_client - INFO - Retrying request to /chat/completions in 27.042000 seconds
2024-03-15 16:27:03,817 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:27:05,156 - root - INFO - Received message: The Bray-Curtis distance has been calculated and stored in the variable `bray_curtis_distance`.
2024-03-15 16:27:05,186 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:27:05] "POST /send_message HTTP/1.1" 200 -
2024-03-15 16:29:50,095 - root - INFO - Received message: Çë¶ÔÎÄ¼þÖÐµÄÊý¾Ý½øÐÐ¦Â·ÖÎö£¬¼ÆËãBray-Curtis¾àÀë£¬²¢Ö±½ÓÕ¹Ê¾Bray-Curtis¾àÀë
2024-03-15 16:29:52,246 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:29:59,338 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:30:43,471 - root - INFO - Received message: The Bray-Curtis distance matrix for the data in the dataframe is as follows:

[[0.         0.26885119 0.35824748 0.51108849 0.35113882 0.50834232
  0.37483408 0.62671068 0.54462272 0.37460325 0.38287627 0.45526588]
 [0.26885119 0.         0.40595945 0.52876395 0.50278127 0.48392687
  0.53667211 0.62564163 0.5367213  0.52949583 0.53075384 0.61374731]
 [0.35824748 0.40595945 0.         0.49803519 0.3759299  0.60824722
  0.50842919 0.64473295 0.58121821 0.42446732 0.48086501 0.51338583]
 [0.51108849 0.52876395 0.49803519 0.         0.532291   0.43940322
  0.46776413 0.52234717 0.45333962 0.57166387 0.57378552 0.64381377]
 [0.35113882 0.50278127 0.3759299  0.532291   0.         0.6851879
  0.52623402 0.74052838 0.68962255 0.45426275 0.43964775 0.45056769]
 [0.50834232 0.48392687 0.60824722 0.43940322 0.6851879  0.
  0.56057845 0.59961622 0.49283646 0.6248323  0.60666939 0.71777031]
 [0.37483408 0.53667211 0.50842919 0.46776413 0.52623402 0.56057845
  0.         0.32533857 0.23189637 0.22988645 0.28548258 0.37873824]
 [0.62671068 0.62564163 0.64473295 0.52234717 0.74052838 0.59961622
  0.32533857 0.         0.21695645 0.4620632  0.50586604 0.61749218]
 [0.54462272 0.5367213  0.58121821 0.45333962 0.68962255 0.49283646
  0.23189637 0.21695645 0.         0.38299787 0.43630893 0.56751138]
 [0.37460325 0.52949583 0.42446732 0.57166387 0.45426275 0.6248323
  0.22988645 0.4620632  0.38299787 0.         0.15424432 0.25741991]
 [0.38287627 0.53075384 0.48086501 0.57378552 0.43964775 0.60666939
  0.28548258 0.50586604 0.43630893 0.15424432 0.         0.26154816]
 [0.45526588 0.61374731 0.51338583 0.64381377 0.45056769 0.71777031
  0.37873824 0.61749218 0.56751138 0.25741991 0.26154816 0.        ]]
2024-03-15 16:30:43,501 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:30:43] "POST /send_message HTTP/1.1" 200 -
2024-03-15 16:39:54,115 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾Ý½øÐÐ¦Â·ÖÎö£¬¼ÆËãÕý³£ÊóºÍ·ÊÅÖÊóÖ®¼äµÄBray-Curtis¾àÀë£¬²¢Ö±½ÓÕ¹Ê¾Bray-Curtis¾àÀë
2024-03-15 16:39:56,966 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:40:05,178 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:40:44,886 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:40:56,706 - root - INFO - Received message: The Bray-Curtis distances between the normal and obese mice are as follows:

[[0.38329941 0.63175483 0.55182227 0.38241296 0.39050541 0.46132606]
 [0.54375157 0.63134821 0.54516797 0.53606014 0.53722736 0.61848753]
 [0.51432975 0.6489895  0.58697784 0.43089509 0.48661087 0.51828399]
 [0.47390375 0.52784735 0.46052407 0.5762737  0.57833283 0.64728112]
 [0.53159661 0.74346024 0.69361469 0.46003312 0.44552228 0.45583127]
 [0.56594839 0.60449989 0.49995648 0.6290927  0.6110953  0.72065411]]
2024-03-15 16:40:56,736 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:40:56] "POST /send_message HTTP/1.1" 200 -
2024-03-15 16:42:56,957 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾Ý½øÐÐ¦Á¶àÑùÐÔ·ÖÎö£¬²¢Êä³öÆäObserved-OTUsÖ¸Êý
2024-03-15 16:42:59,150 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:43:06,726 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:43:09,350 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:43:09,351 - openai._base_client - INFO - Retrying request to /chat/completions in 25.704000 seconds
2024-03-15 16:43:36,951 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:43:39,971 - root - INFO - Received message: The Observed-OTUs index for each sample has been calculated and added to the dataframe. The first five values are 80, 87, 69, 79, and 66.
2024-03-15 16:43:40,005 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:43:40] "POST /send_message HTTP/1.1" 200 -
2024-03-15 16:46:31,868 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾Ý½øÐÐ¦Á¶àÑùÐÔ·ÖÎö£¬²¢È«²¿Êä³öÆäObserved-OTUsÖ¸Êý
2024-03-15 16:46:33,919 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:46:41,045 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:46:46,387 - root - INFO - Received message: The Observed-OTUs indices for the samples are as follows:
0     80
1     87
2     69
3     79
4     66
5     72
6     78
7     76
8     68
9     77
10    82
11    78
2024-03-15 16:46:46,426 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:46:46] "POST /send_message HTTP/1.1" 200 -
2024-03-15 16:50:15,208 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃËæ»úÉ­ÁÖËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 16:50:17,432 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:50:50,517 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:51:00,338 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:51:03,037 - root - INFO - Received message: The bacteria that most influence whether a mouse is obese or not, in descending order of importance, are: Rikenellaceae_RC9_gut_group, Family_XIII_AD3011_group, ventriosum_group, Mucispirillum, and Parabacteroides.
2024-03-15 16:51:03,077 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:51:03] "POST /send_message HTTP/1.1" 200 -
2024-03-15 16:53:40,629 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 16:53:42,565 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:53:57,070 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:54:04,885 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 16:54:04,887 - openai._base_client - INFO - Retrying request to /chat/completions in 18.678000 seconds
2024-03-15 16:54:25,348 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 16:54:34,055 - root - INFO - Received message: The bacteria that have the most influence on mouse obesity are Alloprevotella, Alistipes, and brachy_group. The bacteria that have the least influence are Ruminiclostridium_5, Butyricicoccus, xylanophilum_group, Ruminiclostridium_9, and Defluviitaleaceae_UCG-011.
2024-03-15 16:54:34,096 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 16:54:34] "POST /send_message HTTP/1.1" 200 -
2024-03-15 16:59:04,865 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃSVM-RFEËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 16:59:41,305 - openai._base_client - INFO - Retrying request to /chat/completions in 0.862025 seconds
2024-03-15 17:00:27,164 - openai._base_client - INFO - Retrying request to /chat/completions in 1.808542 seconds
2024-03-15 17:00:28,988 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 122, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_backends\sync.py", line 205, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [Errno 11001] getaddrinfo failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 918, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [Errno 11001] getaddrinfo failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 80, in send_message
    answer = chat_with_csv_agent(message)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 96, in chat_with_csv_agent
    return agent.run(q)
           ^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\chains\base.py", line 545, in run
    return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\chains\base.py", line 378, in __call__
    return self.invoke(
           ^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\chains\base.py", line 163, in invoke
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\chains\base.py", line 153, in invoke
    self._call(inputs, run_manager=run_manager)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\agents\agent.py", line 1391, in _call
    next_step_output = self._take_next_step(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\agents\agent.py", line 1097, in _take_next_step
    [
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\agents\agent.py", line 1097, in <listcomp>
    [
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\agents\agent.py", line 1125, in _iter_next_step
    output = self.agent.plan(
             ^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\agents\agent.py", line 387, in plan
    for chunk in self.runnable.stream(inputs, config={"callbacks": callbacks}):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 2446, in stream
    yield from self.transform(iter([input]), config, **kwargs)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 2433, in transform
    yield from self._transform_stream_with_config(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 1513, in _transform_stream_with_config
    chunk: Output = context.run(next, iterator)  # type: ignore
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 2397, in _transform
    for output in final_pipeline:
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 1051, in transform
    for chunk in input:
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 4173, in transform
    yield from self.bound.transform(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 1061, in transform
    yield from self.stream(final, config, **kwargs)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 250, in stream
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 234, in stream
    for chunk in self._stream(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 398, in _stream
    for chunk in self.completion_with_retry(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 942, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 952, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-03-15 17:00:29,046 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 17:00:29] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-15 17:05:05,280 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃSVM-RFEËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 17:05:08,635 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:05:16,453 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:05:21,334 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:05:21,336 - openai._base_client - INFO - Retrying request to /chat/completions in 22.776000 seconds
2024-03-15 17:05:59,218 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:06:05,895 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:06:05,896 - openai._base_client - INFO - Retrying request to /chat/completions in 14.298000 seconds
2024-03-15 17:06:24,511 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:06:28,601 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:06:28,603 - openai._base_client - INFO - Retrying request to /chat/completions in 28.656000 seconds
2024-03-15 17:06:59,641 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:07:06,708 - root - INFO - Received message: The top 5 most important features (bacteria) that affect mouse obesity are Alloprevotella, Lachnospiraceae_NK4A136_group, Coriobacteriaceae_UCG-002, Mucispirillum, and Bifidobacterium.
2024-03-15 17:07:06,755 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 17:07:06] "POST /send_message HTTP/1.1" 200 -
2024-03-15 17:13:04,888 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾Ý½øÐÐ¦Á¶àÑùÐÔ·ÖÎö£¬²¢Êä³öÆäshannonÖ¸Êý
2024-03-15 17:13:08,598 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:13:15,655 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:13:17,318 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:13:17,319 - openai._base_client - INFO - Retrying request to /chat/completions in 25.374000 seconds
2024-03-15 17:13:47,101 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:13:50,982 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:13:50,984 - openai._base_client - INFO - Retrying request to /chat/completions in 25.752000 seconds
2024-03-15 17:14:19,424 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:14:23,555 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:14:23,557 - openai._base_client - INFO - Retrying request to /chat/completions in 28.518000 seconds
2024-03-15 17:15:06,731 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:15:10,418 - root - INFO - Received message: ÄãÎªÊ²Ã´Ã»ÓÐnumpy
2024-03-15 17:15:10,870 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:15:10,872 - openai._base_client - INFO - Retrying request to /chat/completions in 17.208000 seconds
2024-03-15 17:15:25,617 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:15:31,657 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:15:31,658 - openai._base_client - INFO - Retrying request to /chat/completions in 28.416000 seconds
2024-03-15 17:15:31,981 - root - INFO - Received message: Without a specific context or problem, it's hard to provide a specific answer. Numpy is a library in Python that is used for numerical computations. If the operations in the dataframe do not require numerical computations or if pandas library is sufficient for the data manipulation tasks, numpy may not be necessary.
2024-03-15 17:15:32,033 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 17:15:32] "POST /send_message HTTP/1.1" 200 -
2024-03-15 17:16:03,092 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:16:08,513 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:16:08,514 - openai._base_client - INFO - Retrying request to /chat/completions in 28.596000 seconds
2024-03-15 17:16:52,136 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:16:56,430 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:16:56,431 - openai._base_client - INFO - Retrying request to /chat/completions in 17.862000 seconds
2024-03-15 17:17:16,969 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:17:21,589 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:17:21,590 - openai._base_client - INFO - Retrying request to /chat/completions in 30.738000 seconds
2024-03-15 17:17:55,733 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:18:00,543 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:18:00,544 - openai._base_client - INFO - Retrying request to /chat/completions in 30.768000 seconds
2024-03-15 17:18:31,711 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾Ý½øÐÐ¦Á¶àÑùÐÔ·ÖÎö£¬²¢Êä³öÆäshannonÖ¸Êý
2024-03-15 17:18:33,926 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:18:33,927 - openai._base_client - INFO - Retrying request to /chat/completions in 25.050000 seconds
2024-03-15 17:18:40,792 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:19:02,605 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:19:06,395 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:19:06,396 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:19:06,397 - openai._base_client - INFO - Retrying request to /chat/completions in 32.184000 seconds
2024-03-15 17:19:06,397 - openai._base_client - INFO - Retrying request to /chat/completions in 25.956000 seconds
2024-03-15 17:19:36,394 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:19:41,942 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:19:41,943 - openai._base_client - INFO - Retrying request to /chat/completions in 24.624000 seconds
2024-03-15 17:19:52,295 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:19:52,296 - openai._base_client - INFO - Retrying request to /chat/completions in 19.488000 seconds
2024-03-15 17:20:09,161 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:20:13,251 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:20:13,252 - openai._base_client - INFO - Retrying request to /chat/completions in 28.122000 seconds
2024-03-15 17:20:14,935 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:20:14,937 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 80, in send_message
    answer = chat_with_csv_agent(message)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 96, in chat_with_csv_agent
    return agent.run(q)
           ^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\chains\base.py", line 545, in run
    return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\chains\base.py", line 378, in __call__
    return self.invoke(
           ^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\chains\base.py", line 163, in invoke
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\chains\base.py", line 153, in invoke
    self._call(inputs, run_manager=run_manager)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\agents\agent.py", line 1391, in _call
    next_step_output = self._take_next_step(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\agents\agent.py", line 1097, in _take_next_step
    [
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\agents\agent.py", line 1097, in <listcomp>
    [
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\agents\agent.py", line 1125, in _iter_next_step
    output = self.agent.plan(
             ^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\agents\agent.py", line 387, in plan
    for chunk in self.runnable.stream(inputs, config={"callbacks": callbacks}):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 2446, in stream
    yield from self.transform(iter([input]), config, **kwargs)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 2433, in transform
    yield from self._transform_stream_with_config(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 1513, in _transform_stream_with_config
    chunk: Output = context.run(next, iterator)  # type: ignore
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 2397, in _transform
    for output in final_pipeline:
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 1051, in transform
    for chunk in input:
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 4173, in transform
    yield from self.bound.transform(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 1061, in transform
    yield from self.stream(final, config, **kwargs)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 250, in stream
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 234, in stream
    for chunk in self._stream(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 398, in _stream
    for chunk in self.completion_with_retry(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 965, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 965, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 980, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4 in organization org-vyNfKeAe8rrGqkSwXBJ08UEZ on tokens per min (TPM): Limit 10000, Used 9394, Requested 5754. Please try again in 30.888s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}
2024-03-15 17:20:14,945 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 17:20:14] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-15 17:20:43,297 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:20:48,510 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:20:48,511 - openai._base_client - INFO - Retrying request to /chat/completions in 28.218000 seconds
2024-03-15 17:21:20,844 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 17:21:25,048 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-15 17:21:25,049 - openai._base_client - INFO - Retrying request to /chat/completions in 27.948000 seconds
2024-03-15 18:41:56,086 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 18:41:56,086 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 18:41:56,978 - root - INFO - Rendering index page with 49 chat history records
2024-03-15 18:41:56,984 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 18:41:56] "GET / HTTP/1.1" 200 -
2024-03-15 18:42:21,312 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 18:42:21,312 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 18:42:25,429 - root - INFO - Rendering index page with 49 chat history records
2024-03-15 18:42:25,434 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 18:42:25] "GET / HTTP/1.1" 200 -
2024-03-15 18:42:56,865 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 18:42:56,865 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 18:42:57,285 - root - INFO - Rendering index page with 49 chat history records
2024-03-15 18:42:57,291 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 18:42:57] "GET / HTTP/1.1" 200 -
2024-03-15 18:58:54,624 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 18:58:54,624 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 18:59:02,850 - root - INFO - Rendering index page with 49 chat history records
2024-03-15 18:59:02,855 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 18:59:02] "GET / HTTP/1.1" 200 -
2024-03-15 18:59:08,759 - root - INFO - Received message: ÄãºÃ
2024-03-15 18:59:51,095 - openai._base_client - INFO - Retrying request to /chat/completions in 0.916947 seconds
2024-03-15 19:00:34,109 - openai._base_client - INFO - Retrying request to /chat/completions in 1.985140 seconds
2024-03-15 19:00:48,139 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 122, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_backends\sync.py", line 205, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [Errno 11001] getaddrinfo failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 918, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [Errno 11001] getaddrinfo failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 84, in send_message
    answer = llm.predict(message)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 729, in predict
    result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 691, in __call__
    generation = self.generate(
                 ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 408, in generate
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 398, in generate
    self._generate_with_cache(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 577, in _generate_with_cache
    return self._generate(
           ^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 441, in _generate
    response = self.completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 952, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-03-15 19:00:48,156 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:00:48] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-15 19:00:54,605 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:00:54,606 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:00:59,676 - root - INFO - Rendering index page with 49 chat history records
2024-03-15 19:00:59,681 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:00:59] "GET / HTTP/1.1" 200 -
2024-03-15 19:01:22,078 - root - INFO - Received message: ÄãºÃ
2024-03-15 19:01:34,299 - openai._base_client - INFO - Retrying request to /chat/completions in 0.758799 seconds
2024-03-15 19:01:47,110 - openai._base_client - INFO - Retrying request to /chat/completions in 1.644073 seconds
2024-03-15 19:01:48,758 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 122, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_backends\sync.py", line 205, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [Errno 11001] getaddrinfo failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 918, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [Errno 11001] getaddrinfo failed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 84, in send_message
    answer = llm.predict(message)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 729, in predict
    result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 691, in __call__
    generation = self.generate(
                 ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 408, in generate
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 398, in generate
    self._generate_with_cache(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 577, in _generate_with_cache
    return self._generate(
           ^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 441, in _generate
    response = self.completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 942, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 942, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 952, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-03-15 19:01:48,771 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:01:48] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-15 19:02:10,096 - root - INFO - Received message: ÄãºÃ
2024-03-15 19:02:23,308 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 19:02:23,321 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:02:23] "POST /send_message HTTP/1.1" 200 -
2024-03-15 19:03:05,743 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:03:05,744 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:03:14,023 - root - INFO - Rendering index page with 50 chat history records
2024-03-15 19:03:14,035 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:03:14] "GET / HTTP/1.1" 200 -
2024-03-15 19:03:20,527 - root - INFO - Received message: ÄãºÃ
2024-03-15 19:03:22,689 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 19:03:22,705 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:03:22] "POST /send_message HTTP/1.1" 200 -
2024-03-15 19:04:58,982 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:04:58,982 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:05:02,328 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:05:02,334 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:05:02] "GET / HTTP/1.1" 200 -
2024-03-15 19:06:37,549 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:06:37,549 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:06:40,116 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:06:40,121 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:06:40] "GET / HTTP/1.1" 200 -
2024-03-15 19:06:59,814 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:06:59,814 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:07:02,721 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:07:02,727 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:07:02] "GET / HTTP/1.1" 200 -
2024-03-15 19:13:47,955 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-15 19:13:47,956 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:13:47] "POST /upload_file HTTP/1.1" 200 -
2024-03-15 19:15:44,796 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:15:44,797 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:15:47,485 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:15:47,490 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:15:47] "GET / HTTP/1.1" 200 -
2024-03-15 19:15:52,818 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-15 19:15:52,819 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:15:52] "POST /upload_file HTTP/1.1" 200 -
2024-03-15 19:16:07,177 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:16:07,177 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:16:11,975 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:16:11,981 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:16:11] "GET / HTTP/1.1" 200 -
2024-03-15 19:16:17,559 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-15 19:16:17,560 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:16:17] "POST /upload_file HTTP/1.1" 200 -
2024-03-15 19:19:06,001 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:19:06,001 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:19:10,045 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:19:10,050 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:19:10] "GET / HTTP/1.1" 200 -
2024-03-15 19:19:14,203 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-15 19:19:14,205 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:19:14] "POST /upload_file HTTP/1.1" 200 -
2024-03-15 19:19:37,051 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾Ý½øÐÐ¦Á¶àÑùÐÔ·ÖÎö£¬²¢Êä³öÆäshannonÖ¸Êý
2024-03-15 19:19:37,677 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-15 19:19:37,678 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-15 19:23:10,259 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:23:10,259 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:23:13,345 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:23:13,350 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:23:13] "GET / HTTP/1.1" 200 -
2024-03-15 19:23:56,890 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:23:56,891 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:23:59,432 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:23:59,437 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:23:59] "GET / HTTP/1.1" 200 -
2024-03-15 19:24:06,008 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾Ý½øÐÐ¦Á¶àÑùÐÔ·ÖÎö£¬²¢Êä³öÆäshannonÖ¸Êý
2024-03-15 19:24:48,387 - openai._base_client - INFO - Retrying request to /chat/completions in 0.758374 seconds
2024-03-15 19:25:31,236 - openai._base_client - INFO - Retrying request to /chat/completions in 1.733342 seconds
2024-03-15 19:26:15,094 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 122, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_backends\sync.py", line 205, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 918, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 84, in send_message
    answer = llm.predict(message)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 729, in predict
    result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 691, in __call__
    generation = self.generate(
                 ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 408, in generate
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 398, in generate
    self._generate_with_cache(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 577, in _generate_with_cache
    return self._generate(
           ^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 441, in _generate
    response = self.completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 937, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-03-15 19:26:15,111 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:26:15] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-15 19:26:59,482 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:26:59,482 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:27:04,366 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:27:04,371 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:27:04] "GET / HTTP/1.1" 200 -
2024-03-15 19:27:15,161 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾Ý½øÐÐ¦Á¶àÑùÐÔ·ÖÎö£¬²¢Êä³öÆäshannonÖ¸Êý
2024-03-15 19:28:00,028 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:28:00,028 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:28:07,956 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:28:07,961 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:28:07] "GET / HTTP/1.1" 200 -
2024-03-15 19:28:11,118 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾Ý½øÐÐ¦Á¶àÑùÐÔ·ÖÎö£¬²¢Êä³öÆäshannonÖ¸Êý
2024-03-15 19:28:53,330 - openai._base_client - INFO - Retrying request to /chat/completions in 0.813920 seconds
2024-03-15 19:29:14,762 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:29:14,763 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:29:20,032 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:29:20,040 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:29:20] "GET / HTTP/1.1" 200 -
2024-03-15 19:29:26,328 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾Ý½øÐÐ¦Á¶àÑùÐÔ·ÖÎö£¬²¢Êä³öÆäshannonÖ¸Êý
2024-03-15 19:29:49,900 - openai._base_client - INFO - Retrying request to /chat/completions in 0.769544 seconds
2024-03-15 19:30:14,045 - openai._base_client - INFO - Retrying request to /chat/completions in 1.517937 seconds
2024-03-15 19:30:39,040 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 122, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_backends\sync.py", line 205, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [WinError 10061] ÓÉÓÚÄ¿±ê¼ÆËã»ú»ý¼«¾Ü¾ø£¬ÎÞ·¨Á¬½Ó¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 918, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [WinError 10061] ÓÉÓÚÄ¿±ê¼ÆËã»ú»ý¼«¾Ü¾ø£¬ÎÞ·¨Á¬½Ó¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 84, in send_message
    answer = llm.predict(message)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 729, in predict
    result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 691, in __call__
    generation = self.generate(
                 ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 408, in generate
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 398, in generate
    self._generate_with_cache(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 577, in _generate_with_cache
    return self._generate(
           ^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 441, in _generate
    response = self.completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 942, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 942, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 952, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2024-03-15 19:30:39,055 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:30:39] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-15 19:30:52,485 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:30:52,486 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:30:55,239 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:30:55,245 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:30:55] "GET / HTTP/1.1" 200 -
2024-03-15 19:31:06,192 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾Ý½øÐÐ¦Á¶àÑùÐÔ·ÖÎö£¬²¢Êä³öÆäshannonÖ¸Êý
2024-03-15 19:31:48,535 - openai._base_client - INFO - Retrying request to /chat/completions in 0.786393 seconds
2024-03-15 19:32:22,580 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:32:22,580 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:32:22] "GET / HTTP/1.1" 200 -
2024-03-15 19:32:31,397 - openai._base_client - INFO - Retrying request to /chat/completions in 1.849913 seconds
2024-03-15 19:33:15,459 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 122, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_backends\sync.py", line 205, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 918, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 84, in send_message
    answer = llm.predict(message)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 729, in predict
    result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 691, in __call__
    generation = self.generate(
                 ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 408, in generate
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 398, in generate
    self._generate_with_cache(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 577, in _generate_with_cache
    return self._generate(
           ^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 441, in _generate
    response = self.completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 937, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-03-15 19:33:15,473 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:33:15] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-15 19:33:20,159 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:33:20,159 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:33:25,487 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:33:25,493 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:33:25] "GET / HTTP/1.1" 200 -
2024-03-15 19:33:28,682 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾Ý½øÐÐ¦Á¶àÑùÐÔ·ÖÎö£¬²¢Êä³öÆäshannonÖ¸Êý
2024-03-15 19:33:57,340 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:33:57,341 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:34:07,811 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:34:07,816 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:34:07] "GET / HTTP/1.1" 200 -
2024-03-15 19:34:10,165 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:34:52,465 - openai._base_client - INFO - Retrying request to /chat/completions in 0.849258 seconds
2024-03-15 19:34:58,532 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:34:58,533 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:35:07,790 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:35:07,799 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:35:07] "GET / HTTP/1.1" 200 -
2024-03-15 19:35:15,384 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:35:44,150 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:35:44,150 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:35:50,345 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:35:50,350 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:35:50] "GET / HTTP/1.1" 200 -
2024-03-15 19:35:54,485 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃËæ»úÉ­ÁÖËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:36:36,862 - openai._base_client - INFO - Retrying request to /chat/completions in 0.807564 seconds
2024-03-15 19:37:19,830 - openai._base_client - INFO - Retrying request to /chat/completions in 1.901983 seconds
2024-03-15 19:38:03,825 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 122, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_backends\sync.py", line 205, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 918, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 84, in send_message
    answer = llm.predict(message)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 729, in predict
    result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 691, in __call__
    generation = self.generate(
                 ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 408, in generate
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 398, in generate
    self._generate_with_cache(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 577, in _generate_with_cache
    return self._generate(
           ^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 441, in _generate
    response = self.completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 937, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-03-15 19:38:03,838 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:38:03] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-15 19:38:30,075 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:38:30,076 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:38:36,482 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:38:36,488 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:38:36] "GET / HTTP/1.1" 200 -
2024-03-15 19:38:39,719 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:39:22,054 - openai._base_client - INFO - Retrying request to /chat/completions in 0.814954 seconds
2024-03-15 19:39:47,262 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:39:47,262 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:39:51,311 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:39:51,316 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:39:51] "GET / HTTP/1.1" 200 -
2024-03-15 19:40:02,108 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:40:44,484 - openai._base_client - INFO - Retrying request to /chat/completions in 0.821935 seconds
2024-03-15 19:40:53,582 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:40:53,582 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:40:56,290 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:40:56,296 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:40:56] "GET / HTTP/1.1" 200 -
2024-03-15 19:41:03,438 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:41:46,047 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:41:46,048 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:41:46,271 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:41:46,278 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:41:46] "GET / HTTP/1.1" 200 -
2024-03-15 19:41:49,362 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:42:31,663 - openai._base_client - INFO - Retrying request to /chat/completions in 0.914196 seconds
2024-03-15 19:42:49,660 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:42:49,661 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:42:50,383 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:42:50,388 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:42:50] "GET / HTTP/1.1" 200 -
2024-03-15 19:42:59,246 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:43:39,334 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:43:39,335 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:43:42,198 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:43:42,203 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:43:42] "GET / HTTP/1.1" 200 -
2024-03-15 19:43:49,509 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:44:29,935 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:44:29,935 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:44:32,824 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:44:32,829 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:44:32] "GET / HTTP/1.1" 200 -
2024-03-15 19:44:45,666 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:45:14,683 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:45:14,684 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:45:14,792 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:45:14,797 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:45:14] "GET / HTTP/1.1" 200 -
2024-03-15 19:45:17,475 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:45:24,250 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:45:59,698 - openai._base_client - INFO - Retrying request to /chat/completions in 0.850425 seconds
2024-03-15 19:46:06,170 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:46:06,170 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:46:10,265 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:46:10,270 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:46:10] "GET / HTTP/1.1" 200 -
2024-03-15 19:46:13,398 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:46:55,678 - openai._base_client - INFO - Retrying request to /chat/completions in 0.867355 seconds
2024-03-15 19:47:38,649 - openai._base_client - INFO - Retrying request to /chat/completions in 1.559862 seconds
2024-03-15 19:47:48,874 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:47:48,876 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:47:48,922 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:47:48,927 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:47:48] "GET / HTTP/1.1" 200 -
2024-03-15 19:47:55,100 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:48:22,507 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:48:22,508 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:48:22,595 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:48:22,601 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:48:22] "GET / HTTP/1.1" 200 -
2024-03-15 19:48:25,215 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:49:07,284 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:49:07,284 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:49:07,547 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:49:07,553 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:49:07] "GET / HTTP/1.1" 200 -
2024-03-15 19:49:10,600 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:49:11,141 - openai._base_client - INFO - Retrying request to /chat/completions in 0.872053 seconds
2024-03-15 19:49:12,128 - openai._base_client - INFO - Retrying request to /chat/completions in 1.832888 seconds
2024-03-15 19:49:16,049 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:49:19,999 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:49:23,563 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 19:56:09,270 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 19:56:09,270 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 19:56:09,949 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 19:56:09,954 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 19:56:09] "GET / HTTP/1.1" 200 -
2024-03-15 20:01:53,853 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 20:01:53,854 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 20:01:57,654 - root - INFO - Rendering index page with 51 chat history records
2024-03-15 20:01:57,660 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:01:57] "GET / HTTP/1.1" 200 -
2024-03-15 20:02:47,807 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 20:02:47,808 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 20:02:55,035 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 20:02:55,036 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 20:03:15,764 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-15 20:03:15,765 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:03:15] "POST /upload_file HTTP/1.1" 200 -
2024-03-15 20:03:20,667 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾Ý½øÐÐ¦Á¶àÑùÐÔ·ÖÎö£¬²¢È«²¿Êä³öÆäObserved-OTUsÖ¸Êý
2024-03-15 20:03:21,035 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-15 20:03:21,036 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-15 20:03:24,166 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 20:03:32,191 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-15 20:03:37,311 - root - INFO - Received message: The Observed-OTUs index for each sample is as follows:

0     80
1     87
2     69
3     79
4     66
5     72
6     77
7     75
8     67
9     76
10    81
11    77
2024-03-15 20:03:37,321 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:03:37] "POST /send_message HTTP/1.1" 200 -
2024-03-15 20:05:25,417 - root - INFO - Received message: ÄãºÃ
2024-03-15 20:06:07,693 - openai._base_client - INFO - Retrying request to /chat/completions in 0.853269 seconds
2024-03-15 20:06:15,098 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 20:06:15,099 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 20:06:16,763 - root - INFO - Received message: ÄãºÃ
2024-03-15 20:06:19,365 - root - INFO - Rendering index page with 52 chat history records
2024-03-15 20:06:19,370 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:06:19] "GET / HTTP/1.1" 200 -
2024-03-15 20:06:22,994 - root - INFO - Received message: ÄãºÃ
2024-03-15 20:06:40,775 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃËæ»úÉ­ÁÖËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 20:06:59,018 - openai._base_client - INFO - Retrying request to /chat/completions in 0.786429 seconds
2024-03-15 20:07:05,109 - openai._base_client - INFO - Retrying request to /chat/completions in 0.850885 seconds
2024-03-15 20:07:22,904 - openai._base_client - INFO - Retrying request to /chat/completions in 0.784298 seconds
2024-03-15 20:07:27,840 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ
2024-03-15 20:07:41,980 - openai._base_client - INFO - Retrying request to /chat/completions in 1.716451 seconds
2024-03-15 20:07:48,036 - openai._base_client - INFO - Retrying request to /chat/completions in 1.724749 seconds
2024-03-15 20:08:09,592 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 20:08:09,593 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 20:08:09,785 - root - INFO - Rendering index page with 52 chat history records
2024-03-15 20:08:09,790 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:08:09] "GET / HTTP/1.1" 200 -
2024-03-15 20:08:13,472 - root - INFO - Received message: ÄãºÃ
2024-03-15 20:08:55,774 - openai._base_client - INFO - Retrying request to /chat/completions in 0.752450 seconds
2024-03-15 20:09:38,656 - openai._base_client - INFO - Retrying request to /chat/completions in 1.688598 seconds
2024-03-15 20:10:22,451 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 122, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_backends\sync.py", line 205, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 918, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 84, in send_message
    answer = llm.predict(message)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 729, in predict
    result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 691, in __call__
    generation = self.generate(
                 ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 408, in generate
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 398, in generate
    self._generate_with_cache(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 577, in _generate_with_cache
    return self._generate(
           ^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 441, in _generate
    response = self.completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 937, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-03-15 20:10:22,463 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:10:22] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-15 20:14:47,256 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 20:14:47,257 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 20:14:50,890 - root - INFO - Rendering index page with 52 chat history records
2024-03-15 20:14:50,895 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:14:50] "GET / HTTP/1.1" 200 -
2024-03-15 20:14:55,204 - root - INFO - Received message: ÄãºÃ
2024-03-15 20:15:07,657 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 20:15:37,521 - openai._base_client - INFO - Retrying request to /chat/completions in 0.753328 seconds
2024-03-15 20:15:49,758 - openai._base_client - INFO - Retrying request to /chat/completions in 0.887910 seconds
2024-03-15 20:16:20,345 - openai._base_client - INFO - Retrying request to /chat/completions in 1.523800 seconds
2024-03-15 20:16:29,923 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 20:16:29,924 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 20:16:35,080 - root - INFO - Rendering index page with 52 chat history records
2024-03-15 20:16:35,084 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:16:35] "GET / HTTP/1.1" 200 -
2024-03-15 20:16:38,966 - root - INFO - Received message: ÄãºÃ
2024-03-15 20:17:21,227 - openai._base_client - INFO - Retrying request to /chat/completions in 0.872949 seconds
2024-03-15 20:17:30,021 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 20:17:30,021 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 20:17:30,843 - root - INFO - Rendering index page with 52 chat history records
2024-03-15 20:17:30,850 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:17:30] "GET / HTTP/1.1" 200 -
2024-03-15 20:17:36,985 - root - INFO - Received message: ÄãºÃ
2024-03-15 20:18:19,865 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-15 20:18:19,866 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-15 20:18:34,192 - root - INFO - Rendering index page with 52 chat history records
2024-03-15 20:18:34,203 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:18:34] "GET / HTTP/1.1" 200 -
2024-03-15 20:18:39,335 - root - INFO - Received message: ÄãºÃ
2024-03-15 20:18:47,586 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 20:19:16,151 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 20:19:16,428 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 20:19:16,657 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 20:19:16,835 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 20:19:21,634 - openai._base_client - INFO - Retrying request to /chat/completions in 0.776105 seconds
2024-03-15 20:19:29,673 - openai._base_client - INFO - Retrying request to /chat/completions in 0.831316 seconds
2024-03-15 20:19:58,247 - openai._base_client - INFO - Retrying request to /chat/completions in 0.852305 seconds
2024-03-15 20:19:58,496 - openai._base_client - INFO - Retrying request to /chat/completions in 0.939822 seconds
2024-03-15 20:19:58,759 - openai._base_client - INFO - Retrying request to /chat/completions in 0.753392 seconds
2024-03-15 20:19:58,931 - openai._base_client - INFO - Retrying request to /chat/completions in 0.925697 seconds
2024-03-15 20:20:00,988 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃLasso»Ø¹éËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-15 20:20:04,493 - openai._base_client - INFO - Retrying request to /chat/completions in 1.739284 seconds
2024-03-15 20:20:11,422 - root - INFO - Rendering index page with 52 chat history records
2024-03-15 20:20:11,424 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:20:11] "GET / HTTP/1.1" 200 -
2024-03-15 20:20:12,566 - openai._base_client - INFO - Retrying request to /chat/completions in 1.850730 seconds
2024-03-15 20:20:16,583 - root - INFO - Received message: ÄãºÃ
2024-03-15 20:20:41,227 - openai._base_client - INFO - Retrying request to /chat/completions in 1.790199 seconds
2024-03-15 20:20:41,543 - openai._base_client - INFO - Retrying request to /chat/completions in 1.700456 seconds
2024-03-15 20:20:41,606 - openai._base_client - INFO - Retrying request to /chat/completions in 1.543911 seconds
2024-03-15 20:20:41,955 - openai._base_client - INFO - Retrying request to /chat/completions in 1.759540 seconds
2024-03-15 20:20:43,098 - openai._base_client - INFO - Retrying request to /chat/completions in 0.901650 seconds
2024-03-15 20:20:48,316 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 122, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_backends\sync.py", line 205, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 918, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 84, in send_message
    answer = llm.predict(message)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 729, in predict
    result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 691, in __call__
    generation = self.generate(
                 ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 408, in generate
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 398, in generate
    self._generate_with_cache(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 577, in _generate_with_cache
    return self._generate(
           ^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 441, in _generate
    response = self.completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 937, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-03-15 20:20:48,326 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:20:48] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-15 20:20:56,501 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 122, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_backends\sync.py", line 205, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 918, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 84, in send_message
    answer = llm.predict(message)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 729, in predict
    result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 691, in __call__
    generation = self.generate(
                 ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 408, in generate
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 398, in generate
    self._generate_with_cache(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 577, in _generate_with_cache
    return self._generate(
           ^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 441, in _generate
    response = self.completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 937, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-03-15 20:20:56,509 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:20:56] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-15 20:20:58,655 - openai._base_client - INFO - Retrying request to /chat/completions in 0.917458 seconds
2024-03-15 20:21:25,158 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 122, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_backends\sync.py", line 205, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 918, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 84, in send_message
    answer = llm.predict(message)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 729, in predict
    result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 691, in __call__
    generation = self.generate(
                 ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 408, in generate
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 398, in generate
    self._generate_with_cache(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 577, in _generate_with_cache
    return self._generate(
           ^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 441, in _generate
    response = self.completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 937, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-03-15 20:21:25,175 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:21:25] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-15 20:21:25,252 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 122, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_backends\sync.py", line 205, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 918, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 84, in send_message
    answer = llm.predict(message)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 729, in predict
    result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 691, in __call__
    generation = self.generate(
                 ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 408, in generate
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 398, in generate
    self._generate_with_cache(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 577, in _generate_with_cache
    return self._generate(
           ^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 441, in _generate
    response = self.completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 937, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-03-15 20:21:25,269 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:21:25] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-15 20:21:25,331 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 122, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_backends\sync.py", line 205, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 918, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 84, in send_message
    answer = llm.predict(message)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 729, in predict
    result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 691, in __call__
    generation = self.generate(
                 ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 408, in generate
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 398, in generate
    self._generate_with_cache(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 577, in _generate_with_cache
    return self._generate(
           ^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 441, in _generate
    response = self.completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 937, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-03-15 20:21:25,347 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:21:25] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-15 20:21:25,814 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 122, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_backends\sync.py", line 205, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 918, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 84, in send_message
    answer = llm.predict(message)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 729, in predict
    result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 691, in __call__
    generation = self.generate(
                 ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 408, in generate
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 398, in generate
    self._generate_with_cache(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 577, in _generate_with_cache
    return self._generate(
           ^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 441, in _generate
    response = self.completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 937, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-03-15 20:21:25,830 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:21:25] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-15 20:21:26,105 - openai._base_client - INFO - Retrying request to /chat/completions in 1.762100 seconds
2024-03-15 20:21:41,686 - openai._base_client - INFO - Retrying request to /chat/completions in 1.969537 seconds
2024-03-15 20:22:09,961 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 122, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_backends\sync.py", line 205, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 918, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 84, in send_message
    answer = llm.predict(message)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 729, in predict
    result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 691, in __call__
    generation = self.generate(
                 ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 408, in generate
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 398, in generate
    self._generate_with_cache(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 577, in _generate_with_cache
    return self._generate(
           ^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 441, in _generate
    response = self.completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 937, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-03-15 20:22:09,978 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:22:09] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-15 20:22:25,817 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 122, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_backends\sync.py", line 205, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 918, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 84, in send_message
    answer = llm.predict(message)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 729, in predict
    result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 691, in __call__
    generation = self.generate(
                 ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 408, in generate
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 398, in generate
    self._generate_with_cache(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 577, in _generate_with_cache
    return self._generate(
           ^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 441, in _generate
    response = self.completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 937, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-03-15 20:22:25,835 - werkzeug - INFO - 127.0.0.1 - - [15/Mar/2024 20:22:25] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-16 10:43:16,134 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 10:43:16,135 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 10:43:56,629 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 10:43:56,630 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 10:44:02,117 - root - INFO - Rendering index page with 0 chat records
2024-03-16 10:44:02,125 - main - ERROR - Exception on / [GET]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 54, in index
    return render_template('index.html', history_chats=chat_histories)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\templating.py", line 149, in render_template
    template = app.jinja_env.get_or_select_template(template_name_or_list)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 1081, in get_or_select_template
    return self.get_template(template_name_or_list, parent, globals)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 1010, in get_template
    return self._load_template(name, globals)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 969, in _load_template
    template = self.loader.load(self, name, self.make_globals(globals))
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\loaders.py", line 137, in load
    code = environment.compile(source, name, filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 768, in compile
    self.handle_exception(source=source_hint)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 936, in handle_exception
    raise rewrite_traceback_stack(source=source)
  File "D:\python\anaconda\py\langchain\chat2gm_proj\templates\index.html", line 195, in template
    {% endfor %}
jinja2.exceptions.TemplateSyntaxError: Unexpected end of template. Jinja was looking for the following tags: 'endfor' or 'else'. The innermost block that needs to be closed is 'for'.
2024-03-16 10:44:02,134 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 10:44:02] "[35m[1mGET / HTTP/1.1[0m" 500 -
2024-03-16 10:44:03,446 - root - INFO - Rendering index page with 0 chat records
2024-03-16 10:44:03,453 - main - ERROR - Exception on / [GET]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 54, in index
    return render_template('index.html', history_chats=chat_histories)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\templating.py", line 149, in render_template
    template = app.jinja_env.get_or_select_template(template_name_or_list)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 1081, in get_or_select_template
    return self.get_template(template_name_or_list, parent, globals)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 1010, in get_template
    return self._load_template(name, globals)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 969, in _load_template
    template = self.loader.load(self, name, self.make_globals(globals))
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\loaders.py", line 137, in load
    code = environment.compile(source, name, filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 768, in compile
    self.handle_exception(source=source_hint)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 936, in handle_exception
    raise rewrite_traceback_stack(source=source)
  File "D:\python\anaconda\py\langchain\chat2gm_proj\templates\index.html", line 195, in template
    {% endfor %}
jinja2.exceptions.TemplateSyntaxError: Unexpected end of template. Jinja was looking for the following tags: 'endfor' or 'else'. The innermost block that needs to be closed is 'for'.
2024-03-16 10:44:03,463 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 10:44:03] "[35m[1mGET / HTTP/1.1[0m" 500 -
2024-03-16 10:44:57,788 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 10:44:57,788 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 10:45:26,579 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 10:45:26,579 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 10:45:30,861 - root - INFO - Rendering index page with 0 chat records
2024-03-16 10:45:30,868 - main - ERROR - Exception on / [GET]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 54, in index
    return render_template('index.html', history_chats=chat_histories)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\templating.py", line 149, in render_template
    template = app.jinja_env.get_or_select_template(template_name_or_list)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 1081, in get_or_select_template
    return self.get_template(template_name_or_list, parent, globals)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 1010, in get_template
    return self._load_template(name, globals)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 969, in _load_template
    template = self.loader.load(self, name, self.make_globals(globals))
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\loaders.py", line 137, in load
    code = environment.compile(source, name, filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 768, in compile
    self.handle_exception(source=source_hint)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 936, in handle_exception
    raise rewrite_traceback_stack(source=source)
  File "D:\python\anaconda\py\langchain\chat2gm_proj\templates\index.html", line 188, in template
    {{ history[0].message }}
jinja2.exceptions.TemplateSyntaxError: Unexpected end of template. Jinja was looking for the following tags: 'endfor' or 'else'. The innermost block that needs to be closed is 'for'.
2024-03-16 10:45:30,874 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 10:45:30] "[35m[1mGET / HTTP/1.1[0m" 500 -
2024-03-16 10:45:38,539 - root - INFO - Rendering index page with 0 chat records
2024-03-16 10:45:38,546 - main - ERROR - Exception on / [GET]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 54, in index
    return render_template('index.html', history_chats=chat_histories)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\templating.py", line 149, in render_template
    template = app.jinja_env.get_or_select_template(template_name_or_list)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 1081, in get_or_select_template
    return self.get_template(template_name_or_list, parent, globals)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 1010, in get_template
    return self._load_template(name, globals)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 969, in _load_template
    template = self.loader.load(self, name, self.make_globals(globals))
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\loaders.py", line 137, in load
    code = environment.compile(source, name, filename)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 768, in compile
    self.handle_exception(source=source_hint)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 936, in handle_exception
    raise rewrite_traceback_stack(source=source)
  File "D:\python\anaconda\py\langchain\chat2gm_proj\templates\index.html", line 188, in template
    {{ history[0].message }}
jinja2.exceptions.TemplateSyntaxError: Unexpected end of template. Jinja was looking for the following tags: 'endfor' or 'else'. The innermost block that needs to be closed is 'for'.
2024-03-16 10:45:38,551 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 10:45:38] "[35m[1mGET / HTTP/1.1[0m" 500 -
2024-03-16 10:46:21,015 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 10:46:21,016 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 10:46:24,239 - root - INFO - Rendering index page with 0 chat records
2024-03-16 10:46:24,250 - main - ERROR - Exception on / [GET]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 54, in index
    return render_template('index.html', history_chats=chat_histories)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\templating.py", line 150, in render_template
    return _render(app, template, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\templating.py", line 131, in _render
    rv = template.render(context)
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 1301, in render
    self.environment.handle_exception()
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 936, in handle_exception
    raise rewrite_traceback_stack(source=source)
  File "D:\python\anaconda\py\langchain\chat2gm_proj\templates\index.html", line 185, in top-level template code
    {% for chat_id, history in chat_histories.items() %}
    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 485, in getattr
    return getattr(obj, attribute)
           ^^^^^^^^^^^^^^^^^^^^^^^
jinja2.exceptions.UndefinedError: 'chat_histories' is undefined
2024-03-16 10:46:24,256 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 10:46:24] "[35m[1mGET / HTTP/1.1[0m" 500 -
2024-03-16 10:46:25,522 - root - INFO - Rendering index page with 0 chat records
2024-03-16 10:46:25,523 - main - ERROR - Exception on / [GET]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 54, in index
    return render_template('index.html', history_chats=chat_histories)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\templating.py", line 150, in render_template
    return _render(app, template, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\templating.py", line 131, in _render
    rv = template.render(context)
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 1301, in render
    self.environment.handle_exception()
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 936, in handle_exception
    raise rewrite_traceback_stack(source=source)
  File "D:\python\anaconda\py\langchain\chat2gm_proj\templates\index.html", line 185, in top-level template code
    {% for chat_id, history in chat_histories.items() %}
    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 485, in getattr
    return getattr(obj, attribute)
           ^^^^^^^^^^^^^^^^^^^^^^^
jinja2.exceptions.UndefinedError: 'chat_histories' is undefined
2024-03-16 10:46:25,527 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 10:46:25] "[35m[1mGET / HTTP/1.1[0m" 500 -
2024-03-16 10:47:12,081 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 10:47:12,081 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 10:47:15,806 - root - INFO - Rendering index page with 0 chat records
2024-03-16 10:47:15,815 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 10:47:15] "GET / HTTP/1.1" 200 -
2024-03-16 10:47:24,133 - root - INFO - Received message: ÄãºÃ
2024-03-16 10:47:26,549 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 10:47:26,590 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 98, in send_message
    chat_record = ChatHistory(chat_id=chat_id,message=message, response=str(answer))
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 4, in __init__
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\orm\state.py", line 564, in _initialize_instance
    with util.safe_reraise():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\orm\state.py", line 562, in _initialize_instance
    manager.original_init(*mixed[1:], **kwargs)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\orm\decl_base.py", line 2139, in _declarative_constructor
    raise TypeError(
TypeError: 'response' is an invalid keyword argument for ChatHistory
2024-03-16 10:47:26,601 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 10:47:26] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-16 10:50:09,084 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 10:50:09,084 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 10:50:15,351 - root - INFO - Rendering index page with 0 chat records
2024-03-16 10:50:15,366 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 10:50:15] "GET / HTTP/1.1" 200 -
2024-03-16 10:50:20,955 - root - INFO - Received message: ÄãºÃ
2024-03-16 10:50:22,983 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 10:50:23,014 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\engine\base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\engine\default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: table chat_history has no column named response

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 100, in send_message
    db.session.commit()
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\orm\scoping.py", line 597, in commit
    return self._proxied.commit()
           ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\orm\session.py", line 1972, in commit
    trans.commit(_to_root=True)
  File "<string>", line 2, in commit
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\orm\state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
                ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\orm\session.py", line 1257, in commit
    self._prepare_impl()
  File "<string>", line 2, in _prepare_impl
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\orm\state_changes.py", line 139, in _go
    ret_value = fn(self, *arg, **kw)
                ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\orm\session.py", line 1232, in _prepare_impl
    self.session.flush()
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\orm\session.py", line 4296, in flush
    self._flush(objects)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\orm\session.py", line 4431, in _flush
    with util.safe_reraise():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\util\langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\orm\session.py", line 4392, in _flush
    flush_context.execute()
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\orm\unitofwork.py", line 466, in execute
    rec.execute(self)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\orm\unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\orm\persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\orm\persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
             ^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\engine\base.py", line 1421, in execute
    return meth(
           ^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\sql\elements.py", line 514, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\engine\base.py", line 1643, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\engine\base.py", line 1849, in _execute_context
    return self._exec_single_context(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\engine\base.py", line 1989, in _exec_single_context
    self._handle_dbapi_exception(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\engine\base.py", line 2356, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\engine\base.py", line 1970, in _exec_single_context
    self.dialect.do_execute(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\sqlalchemy\engine\default.py", line 924, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) table chat_history has no column named response
[SQL: INSERT INTO chat_history (chat_id, message, response) VALUES (?, ?, ?) RETURNING id, timestamp]
[parameters: (1, 'ÄãºÃ', 'ÄãºÃ£¡ÓÐÊ²Ã´¿ÉÒÔ°ïÖúÄãµÄÂð£¿')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2024-03-16 10:50:23,045 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 10:50:23] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-16 10:57:52,881 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 10:57:52,881 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 10:58:00,552 - root - INFO - Rendering index page with 0 chat records
2024-03-16 10:58:00,561 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 10:58:00] "GET / HTTP/1.1" 200 -
2024-03-16 10:58:06,453 - root - INFO - Received message: ÄãºÃ
2024-03-16 10:58:09,163 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 10:58:09,198 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 10:58:09] "POST /send_message HTTP/1.1" 200 -
2024-03-16 10:58:26,797 - root - INFO - Received message: ÄãÊÇË­
2024-03-16 10:58:28,790 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 10:58:28,824 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 10:58:28] "POST /send_message HTTP/1.1" 200 -
2024-03-16 11:03:18,874 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 11:03:18,875 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 11:03:23,671 - root - INFO - Rendering index page with 2 chat records
2024-03-16 11:03:23,681 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 11:03:23] "GET / HTTP/1.1" 200 -
2024-03-16 11:03:30,739 - root - INFO - Received message: ÄãºÃ
2024-03-16 11:03:32,755 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 11:03:32,797 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 11:03:32] "POST /send_message HTTP/1.1" 200 -
2024-03-16 11:05:57,051 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 11:05:57,052 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 11:05:59,618 - root - INFO - Rendering index page with 3 chat records
2024-03-16 11:05:59,627 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 11:05:59] "GET / HTTP/1.1" 200 -
2024-03-16 11:06:46,158 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 11:06:46,159 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 11:06:49,345 - root - INFO - Rendering index page with 3 chat records
2024-03-16 11:06:49,355 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 11:06:49] "GET / HTTP/1.1" 200 -
2024-03-16 11:09:24,555 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 11:09:24,556 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 11:09:27,233 - root - INFO - Rendering index page with 3 chat records
2024-03-16 11:09:27,244 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 11:09:27] "GET / HTTP/1.1" 200 -
2024-03-16 11:09:54,791 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 11:09:54,791 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 11:09:57,585 - root - INFO - Rendering index page with 3 chat records
2024-03-16 11:09:57,593 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 11:09:57] "GET / HTTP/1.1" 200 -
2024-03-16 11:11:47,346 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 11:11:47,347 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 11:11:47,724 - root - INFO - Rendering index page with 3 chat records
2024-03-16 11:11:47,735 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 11:11:47] "GET / HTTP/1.1" 200 -
2024-03-16 13:30:26,308 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 13:30:26,309 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 13:30:55,935 - root - INFO - Rendering index page with 3 chat records
2024-03-16 13:30:55,943 - main - ERROR - Exception on / [GET]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 55, in index
    return render_template('index.html', chat_histories=chat_histories)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\templating.py", line 150, in render_template
    return _render(app, template, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\templating.py", line 131, in _render
    rv = template.render(context)
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 1301, in render
    self.environment.handle_exception()
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 936, in handle_exception
    raise rewrite_traceback_stack(source=source)
  File "D:\python\anaconda\py\langchain\chat2gm_proj\templates\index.html", line 188, in top-level template code
    {{ history[0].message }}
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 485, in getattr
    return getattr(obj, attribute)
           ^^^^^^^^^^^^^^^^^^^^^^^
jinja2.exceptions.UndefinedError: models.ChatHistory object has no element 0
2024-03-16 13:30:55,948 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 13:30:55] "[35m[1mGET / HTTP/1.1[0m" 500 -
2024-03-16 13:30:57,886 - root - INFO - Rendering index page with 3 chat records
2024-03-16 13:30:57,886 - main - ERROR - Exception on / [GET]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 55, in index
    return render_template('index.html', chat_histories=chat_histories)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\templating.py", line 150, in render_template
    return _render(app, template, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\templating.py", line 131, in _render
    rv = template.render(context)
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 1301, in render
    self.environment.handle_exception()
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 936, in handle_exception
    raise rewrite_traceback_stack(source=source)
  File "D:\python\anaconda\py\langchain\chat2gm_proj\templates\index.html", line 188, in top-level template code
    {{ history[0].message }}
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 485, in getattr
    return getattr(obj, attribute)
           ^^^^^^^^^^^^^^^^^^^^^^^
jinja2.exceptions.UndefinedError: models.ChatHistory object has no element 0
2024-03-16 13:30:57,888 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 13:30:57] "[35m[1mGET / HTTP/1.1[0m" 500 -
2024-03-16 13:30:59,068 - root - INFO - Rendering index page with 3 chat records
2024-03-16 13:30:59,068 - main - ERROR - Exception on / [GET]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 55, in index
    return render_template('index.html', chat_histories=chat_histories)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\templating.py", line 150, in render_template
    return _render(app, template, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\templating.py", line 131, in _render
    rv = template.render(context)
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 1301, in render
    self.environment.handle_exception()
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 936, in handle_exception
    raise rewrite_traceback_stack(source=source)
  File "D:\python\anaconda\py\langchain\chat2gm_proj\templates\index.html", line 188, in top-level template code
    {{ history[0].message }}
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\jinja2\environment.py", line 485, in getattr
    return getattr(obj, attribute)
           ^^^^^^^^^^^^^^^^^^^^^^^
jinja2.exceptions.UndefinedError: models.ChatHistory object has no element 0
2024-03-16 13:30:59,070 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 13:30:59] "[35m[1mGET / HTTP/1.1[0m" 500 -
2024-03-16 13:32:54,430 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 13:32:54,431 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 13:32:57,633 - root - INFO - Rendering index page with 3 chat records
2024-03-16 13:32:57,639 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 13:32:57] "GET / HTTP/1.1" 200 -
2024-03-16 13:33:11,978 - root - INFO - Received message: ÄãºÃ
2024-03-16 13:33:14,119 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 13:33:14,138 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 13:33:14] "POST /send_message HTTP/1.1" 200 -
2024-03-16 13:33:19,241 - root - INFO - Received message: ÄãÊÇË­
2024-03-16 13:33:21,096 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 13:33:21,108 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 13:33:21] "POST /send_message HTTP/1.1" 200 -
2024-03-16 13:57:39,651 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 13:57:39,651 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 13:57:43,574 - root - INFO - Rendering index page with 5 chat records
2024-03-16 13:57:43,580 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 13:57:43] "GET / HTTP/1.1" 200 -
2024-03-16 13:57:50,073 - root - INFO - Received message: ÄãºÃ
2024-03-16 13:57:52,625 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 13:57:52,641 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 13:57:52] "POST /send_message HTTP/1.1" 200 -
2024-03-16 13:58:22,807 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 13:58:22,808 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 13:58:23,506 - root - INFO - Rendering index page with 6 chat records
2024-03-16 13:58:23,512 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 13:58:23] "GET / HTTP/1.1" 200 -
2024-03-16 13:58:28,148 - root - INFO - Received message: ÄãºÃ
2024-03-16 13:58:29,795 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 13:58:29,812 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 13:58:29] "POST /send_message HTTP/1.1" 200 -
2024-03-16 13:58:36,820 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-16 13:58:36,821 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 13:58:36] "POST /upload_file HTTP/1.1" 200 -
2024-03-16 13:58:58,294 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃËæ»úÉ­ÁÖËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-16 13:58:58,845 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-16 13:58:58,846 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-16 13:59:05,022 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 13:59:06,692 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 13:59:08,540 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 13:59:10,649 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 13:59:12,961 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 13:59:14,748 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 13:59:16,661 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 13:59:18,694 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 13:59:20,692 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 13:59:22,439 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 13:59:24,459 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 14:00:42,940 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 14:00:42,940 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 14:00:46,460 - root - INFO - Rendering index page with 0 chat records
2024-03-16 14:00:46,467 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:00:46] "GET / HTTP/1.1" 200 -
2024-03-16 14:00:53,194 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-16 14:00:53,195 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:00:53] "POST /upload_file HTTP/1.1" 200 -
2024-03-16 14:00:58,208 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃËæ»úÉ­ÁÖËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-16 14:00:58,638 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-16 14:00:58,638 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-16 14:01:00,801 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 404 Not Found"
2024-03-16 14:01:00,802 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 86, in send_message
    answer = chat_with_csv_agent(message)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 108, in chat_with_csv_agent
    return agent.run(q)
           ^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\chains\base.py", line 545, in run
    return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\chains\base.py", line 378, in __call__
    return self.invoke(
           ^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\chains\base.py", line 163, in invoke
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\chains\base.py", line 153, in invoke
    self._call(inputs, run_manager=run_manager)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\agents\agent.py", line 1391, in _call
    next_step_output = self._take_next_step(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\agents\agent.py", line 1097, in _take_next_step
    [
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\agents\agent.py", line 1097, in <listcomp>
    [
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\agents\agent.py", line 1125, in _iter_next_step
    output = self.agent.plan(
             ^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain\agents\agent.py", line 387, in plan
    for chunk in self.runnable.stream(inputs, config={"callbacks": callbacks}):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 2446, in stream
    yield from self.transform(iter([input]), config, **kwargs)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 2433, in transform
    yield from self._transform_stream_with_config(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 1513, in _transform_stream_with_config
    chunk: Output = context.run(next, iterator)  # type: ignore
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 2397, in _transform
    for output in final_pipeline:
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 1051, in transform
    for chunk in input:
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 4173, in transform
    yield from self.bound.transform(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\runnables\base.py", line 1061, in transform
    yield from self.stream(final, config, **kwargs)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 250, in stream
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 234, in stream
    for chunk in self._stream(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 398, in _stream
    for chunk in self.completion_with_retry(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 980, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
2024-03-16 14:01:00,824 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:01:00] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-16 14:01:19,685 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 14:01:19,686 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 14:01:23,418 - root - INFO - Rendering index page with 0 chat records
2024-03-16 14:01:23,423 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:01:23] "GET / HTTP/1.1" 200 -
2024-03-16 14:01:29,866 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-16 14:01:29,867 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:01:29] "POST /upload_file HTTP/1.1" 200 -
2024-03-16 14:01:32,343 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºexcelÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃËæ»úÉ­ÁÖËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-16 14:01:32,742 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-16 14:01:32,742 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-16 14:01:35,758 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 14:01:46,725 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 14:01:51,587 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-16 14:01:51,588 - openai._base_client - INFO - Retrying request to /chat/completions in 23.436000 seconds
2024-03-16 14:02:16,829 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 14:02:19,825 - root - INFO - Received message: The top 5 bacteria that have the highest importance are Alloprevotella, Turicibacter, coprostanoligenes_group, Alistipes, and brachy_group.
2024-03-16 14:02:19,836 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:02:19] "POST /send_message HTTP/1.1" 200 -
2024-03-16 14:05:47,665 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 14:05:47,665 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 14:05:55,321 - root - INFO - Rendering index page with 1 chat records
2024-03-16 14:05:55,326 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:05:55] "GET / HTTP/1.1" 200 -
2024-03-16 14:06:02,660 - root - INFO - CSV file 'output.csv' uploaded successfully
2024-03-16 14:06:02,662 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:06:02] "POST /upload_file HTTP/1.1" 200 -
2024-03-16 14:06:15,744 - root - INFO - Received message: ÎÒÒÑÉÏ´«³¦µÀ¾úÈºÊý¾ÝÎÄ¼þ£¬ÎÄ¼þÖÐÓÐÁù¸öÕý³£Ð¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª0£©£¬Áù¸ö·ÊÅÖÐ¡ÊóµÄ³¦µÀ¾úÈºÊý¾Ý£¨±êÇ©ÉèÎª1£©£¬Ã¿¸öÑù±¾¶¼ÓÐÆä³¦µÀ¾úÈºµÄÊý¾Ý£¬°ïÎÒ¶ÔÎÄ¼þÖÐ³¦µÀ¾úÈºÊý¾ÝÊ¹ÓÃËæ»úÉ­ÁÖËã·¨£¬ÕÒ³öÓ°ÏìÐ¡Êó·ÊÅÖµÄÌØÕ÷¾úÈº£¬²¢ÅÅÐò ÇëÈ«²¿Êä³ö
2024-03-16 14:06:16,152 - numexpr.utils - INFO - Note: NumExpr detected 16 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2024-03-16 14:06:16,154 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.
2024-03-16 14:06:18,795 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 14:06:29,594 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 14:06:36,957 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2024-03-16 14:06:36,957 - openai._base_client - INFO - Retrying request to /chat/completions in 22.644000 seconds
2024-03-16 14:07:03,060 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 14:07:08,489 - root - INFO - Received message: The most important bacteria that affect mouse obesity, sorted by importance, are as follows:

1. Prevotellaceae_UCG-001
2. Alistipes
3. Ruminococcaceae_UCG-005
4. ventriosum_group
5. Mucispirillum
...
115. Bacteroides
116. Candidatus_Stoquefichus
117. Helicobacter
118. Coriobacteriaceae_UCG-002
119. Defluviitaleaceae_UCG-011
2024-03-16 14:07:08,503 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:07:08] "POST /send_message HTTP/1.1" 200 -
2024-03-16 14:11:42,014 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 14:11:42,014 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 14:11:47,845 - root - INFO - Rendering index page with 2 chat records
2024-03-16 14:11:47,851 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:11:47] "GET / HTTP/1.1" 200 -
2024-03-16 14:11:54,278 - root - INFO - Received message: ÄãºÃ
2024-03-16 14:11:56,008 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 14:11:56,023 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:11:56] "POST /send_message HTTP/1.1" 200 -
2024-03-16 14:15:45,716 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 14:15:45,717 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 14:15:52,394 - root - INFO - Rendering index page with 3 chat records
2024-03-16 14:15:52,399 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:15:52] "GET / HTTP/1.1" 200 -
2024-03-16 14:16:00,210 - root - INFO - Received message: ÄãºÃ
2024-03-16 14:16:02,614 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 14:16:02,624 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:16:02] "POST /send_message HTTP/1.1" 200 -
2024-03-16 14:19:01,238 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 14:19:01,238 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 14:19:03,875 - root - INFO - Rendering index page with 4 chat records
2024-03-16 14:19:03,882 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:19:03] "GET / HTTP/1.1" 200 -
2024-03-16 14:19:10,520 - root - INFO - Received message: ÄãºÃ
2024-03-16 14:19:12,365 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 14:19:12,382 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:19:12] "POST /send_message HTTP/1.1" 200 -
2024-03-16 14:19:24,237 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 14:19:24,238 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 14:19:26,903 - root - INFO - Rendering index page with 5 chat records
2024-03-16 14:19:26,908 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:19:26] "GET / HTTP/1.1" 200 -
2024-03-16 14:19:33,054 - root - INFO - Received message: ÄãºÃ
2024-03-16 14:19:35,034 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 14:19:35,053 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:19:35] "POST /send_message HTTP/1.1" 200 -
2024-03-16 14:19:45,335 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 14:19:45,336 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 14:19:48,076 - root - INFO - Rendering index page with 6 chat records
2024-03-16 14:19:48,082 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:19:48] "GET / HTTP/1.1" 200 -
2024-03-16 14:19:53,404 - root - INFO - Received message: ÄãºÃ
2024-03-16 14:19:55,508 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 14:19:55,523 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:19:55] "POST /send_message HTTP/1.1" 200 -
2024-03-16 14:20:19,737 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 14:20:19,737 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 14:20:22,223 - root - INFO - Rendering index page with 7 chat records
2024-03-16 14:20:22,230 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:20:22] "GET / HTTP/1.1" 200 -
2024-03-16 14:20:27,220 - root - INFO - Received message: ÄãºÃ
2024-03-16 14:20:28,963 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2024-03-16 14:20:28,979 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:20:28] "POST /send_message HTTP/1.1" 200 -
2024-03-16 14:22:02,982 - root - INFO - Received message: ÄãºÃ
2024-03-16 14:22:04,285 - root - INFO - Received message: ÄãºÃ
2024-03-16 14:22:45,194 - openai._base_client - INFO - Retrying request to /chat/completions in 0.939862 seconds
2024-03-16 14:22:46,371 - openai._base_client - INFO - Retrying request to /chat/completions in 0.923172 seconds
2024-03-16 14:23:28,220 - openai._base_client - INFO - Retrying request to /chat/completions in 1.971712 seconds
2024-03-16 14:23:29,375 - openai._base_client - INFO - Retrying request to /chat/completions in 1.995105 seconds
2024-03-16 14:24:12,352 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 122, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_backends\sync.py", line 205, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 918, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 90, in send_message
    answer = llm.predict(message)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 729, in predict
    result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 691, in __call__
    generation = self.generate(
                 ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 408, in generate
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 398, in generate
    self._generate_with_cache(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 577, in _generate_with_cache
    return self._generate(
           ^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 441, in _generate
    response = self.completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 937, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-03-16 14:24:12,366 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:24:12] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-16 14:24:13,471 - main - ERROR - Exception on /send_message [POST]
Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 69, in map_httpcore_exceptions
    yield
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 216, in handle_request
    raise exc from None
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 99, in handle_request
    raise exc
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 76, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_sync\connection.py", line 122, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_backends\sync.py", line 205, in connect_tcp
    with map_exceptions(exc_map):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 918, in _request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\httpx\_transports\default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectTimeout: [WinError 10060] ÓÉÓÚÁ¬½Ó·½ÔÚÒ»¶ÎÊ±¼äºóÃ»ÓÐÕýÈ·´ð¸´»òÁ¬½ÓµÄÖ÷»úÃ»ÓÐ·´Ó¦£¬Á¬½Ó³¢ÊÔÊ§°Ü¡£

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 1463, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 872, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 870, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\flask\app.py", line 855, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\py\langchain\chat2gm_proj\main.py", line 90, in send_message
    answer = llm.predict(message)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 729, in predict
    result = self([HumanMessage(content=text)], stop=_stop, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\_api\deprecation.py", line 145, in warning_emitting_wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 691, in __call__
    generation = self.generate(
                 ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 408, in generate
    raise e
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 398, in generate
    self._generate_with_cache(
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_core\language_models\chat_models.py", line 577, in _generate_with_cache
    return self._generate(
           ^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 441, in _generate
    response = self.completion_with_retry(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\langchain_community\chat_models\openai.py", line 356, in completion_with_retry
    return self.client.create(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_utils\_utils.py", line 275, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\resources\chat\completions.py", line 663, in create
    return self._post(
           ^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 889, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 927, in _request
    return self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 1013, in _retry_request
    return self._request(
           ^^^^^^^^^^^^^^
  File "D:\python\anaconda\myAnaconda\envs\langchainTest\Lib\site-packages\openai\_base_client.py", line 937, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2024-03-16 14:24:13,482 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:24:13] "[35m[1mPOST /send_message HTTP/1.1[0m" 500 -
2024-03-16 14:26:04,180 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 14:26:04,181 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 14:26:06,723 - root - INFO - Rendering index page with 8 chat records
2024-03-16 14:26:06,729 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:26:06] "GET / HTTP/1.1" 200 -
2024-03-16 14:26:42,272 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 14:26:42,272 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 14:26:44,823 - root - INFO - Rendering index page with 8 chat records
2024-03-16 14:26:44,829 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:26:44] "GET / HTTP/1.1" 200 -
2024-03-16 14:27:58,411 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5000
2024-03-16 14:27:58,411 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2024-03-16 14:28:00,313 - root - INFO - Rendering index page with 8 chat records
2024-03-16 14:28:00,319 - werkzeug - INFO - 127.0.0.1 - - [16/Mar/2024 14:28:00] "GET / HTTP/1.1" 200 -
2024-03-16 14:28:12,687 - root - INFO - Received message: ÄãºÃ
2024-03-16 14:28:55,004 - openai._base_client - INFO - Retrying request to /chat/completions in 0.799816 seconds
